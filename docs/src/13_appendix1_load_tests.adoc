ifndef::imagesdir[:imagesdir: ../images]

[[section-load-tests]]
== Appendix I: Load tests

We conducted load tests on our application using Gatling. This type of testing allows us to know how __strong__ our application is in relation with the amount of users that interact with it at the same time. Initially, we recorded the specific functionalities we intended to test, and then we configured the tests accordingly. Our primary focus was on testing the game component, as it constitutes the core aspect of our application. After recording the functionality to be tested, we increased the number of requests to 1,000 and established that these requests be made gradually, simulating a real-life scenario.

image::13_conf_recording_gatling_1.png["Configuration for gatling recording 1"]

After setting this, me executed the load tests, obtaining results that were not too bad. However, as it is shown in the next picture, more than 25% of the requests failed. This means that there is a possibility that the game fails when playing, which is not acceptable in an application of this kind. Even most of the requests have a response we will try to reduce the amount of failed requests. In addition, instant response of petitions has not the highest priority for us. It is more important that as many requests as possible and responded correctly.

=== Test 1: 1000 users with poor question generation algorithm

image::13_results_gatling_1.png["Results for gatling recording test 1"]

After this load test, we tried to improve the question generation so as to avoid the failures mentioned above.
We tested our application again, obtaining new results. We believe it is important to mention that even if the settings where the same in both tests, the application had more game modalities and new functionalities, which may affect to the number of requests and the time needed for each one.

=== Test 2: 1000 users with new question generation algorithm

image::13_results_gatling_2.png["Results for gatling recording test 2"]

As it can be seen in the picture above, the results have changed noticeably. From our point of view, there are two main aspects which seem remarkable. On one hand, the drastic decrease in the number of failed requests. The failed requests have decreased to 2% compared to 27% in the first test. This demonstrates that the changes made to the application have achieved their objective.
On the other hand, the overall increase in the time it takes to respond to a request catches the eye as well. Nevertheless, given that the difference in time is milliseconds and it is not a real-time critical application, we consider that the objective of these tests has been fulfilled.

=== Users distribution along the simulation and response time distribution
We think that it is useful to compare some of the statistics that these tests have providad us with. For instance, seeing __Users distribution along the simulation__ graphic allows us to see if the stablished settings get to a gradual users interaction with the application. Also, the __Response time distribution__ graphic is very visual so as to see the average time requests take to respond.

==== Test 1
image::13_statistics_1.png["Some statistics for test 1"]

==== Test 2
image::13_statistics_2.png["Some statistics for test 2"]


=== Number of responses per second
Finally, we would like to compare another graph because we believe it illustrates the number of responses per second in a straightforward manner during the tests. This allows us to observe the percentage of failed requests each second. As mentioned earlier, our primary goal after the initial test was to reduce the number of failed requests, even if it meant slightly increasing the average response time for each request.

==== Test 1
image::13_responses_per_seconds_1.png["Responses per second 1"]

==== Test 2
image::13_responses_per_second_2.png["Responses per second 2"]

As we can see, the second test shows a much more equilibrated graphic. Responses are distributed better in time and failures are a minimum percentage of the total responses.

For these reasons, the load tests have motivated us to develop a more stable question generation algorithm. This reduces the likelihood of requests failing when users are interacting with our application. This ultimately leads to a better user experience, which is a crucial aspect of application development.








